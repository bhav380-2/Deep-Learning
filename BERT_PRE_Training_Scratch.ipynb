{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoTokenizer\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import math\n",
        "\n",
        "import math\n",
        "import re\n",
        "from   random import *\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import os"
      ],
      "metadata": {
        "id": "rqKus4ZVGdZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Dataset"
      ],
      "metadata": {
        "id": "7Luni6NpBHxP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"text\", data_files={\"train\": [\"./wiki.train.tokens\"], \"test\": \"./wiki.test.tokens\"})\n"
      ],
      "metadata": {
        "id": "VHPqup_cDvW0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = dataset[\"test\"]\n",
        "train_dataset = dataset[\"train\"]\n",
        "\n",
        "print(test_dataset.shape)\n",
        "print(train_dataset.shape)\n",
        "\n",
        "train_dataset[0:100]"
      ],
      "metadata": {
        "id": "-Z_wehHZErzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clean Dataset"
      ],
      "metadata": {
        "id": "1ncwfGOqBPaF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_data(data):\n",
        "  cleaned_texts = []\n",
        "  for text in data['text']:\n",
        "    x = text.lower()\n",
        "    # remove urls\n",
        "    x = re.sub(r'https?://\\S+|www\\.\\S+', ' ', x)\n",
        "    # fix formattting : \"hello , world\" -> \"hello, world\"\n",
        "    x = re.sub(r'\\s([.,!?\";:])', r'\\1', x)\n",
        "    # Keep basic punctuation but remove weird symbols\n",
        "    x = re.sub(r'[^a-zA-Z0-9.,!? \\n]', '', x)\n",
        "    # collapse multiple spaces/lines into one\n",
        "    x = re.sub(r'\\s+', ' ', x).strip()\n",
        "    cleaned_texts.append(x)\n",
        "\n",
        "  cleaned = []\n",
        "  max_len = 128\n",
        "  sentence = \"\"\n",
        "  for text in cleaned_texts:\n",
        "    if len(sentence.strip())<70:\n",
        "      sentence += \" \"+text\n",
        "    else:\n",
        "      cleaned.append(sentence.strip())\n",
        "      sentence = text\n",
        "  # cleaned.filter(lambda x: len(x)>0)\n",
        "  return {\"text\":cleaned}"
      ],
      "metadata": {
        "id": "sfgvQdApD5pf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(test_dataset),len(train_dataset)"
      ],
      "metadata": {
        "id": "kjZn4SnyJWbX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = test_dataset.map(clean_data,batched=True)\n",
        "train_dataset = train_dataset.map(clean_data,batched=True)\n",
        "len(test_dataset),len(train_dataset)\n"
      ],
      "metadata": {
        "id": "0-oCHPcKJGHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = train_dataset.filter(lambda x: len(x['text'])>0)\n",
        "test_dataset = test_dataset.filter(lambda x : len(x['text'])>0)\n",
        "\n",
        "len(test_dataset),len(train_dataset)"
      ],
      "metadata": {
        "id": "mJBdRZxgJ1Pj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "f9bf51f3-5973-4335-ac3f-417099888f06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Filter:   0%|          | 0/11618 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "06f456f3181f406eae1b2ecde09bfec8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Filter:   0%|          | 0/2471 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "da8972a0a7c9459a8932408cabaab3fc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = test_dataset.map(clean_data,batched=True)\n",
        "train_dataset = train_dataset.map(clean_data,batched=True)\n",
        "len(test_dataset),len(train_dataset)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "id": "WKc-0M8yF-B2",
        "outputId": "15a77559-7df4-4253-a49c-20ec2d7ce06d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/2891 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5874f9b1ca5f4b58b79ed5df331a7a5c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/13695 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "23dabdb669c843c286c3e38fde801f49"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset.to_csv(\"cleaned_train.txt\",columns=[\"text\"],index=False,header=False)\n",
        "test_dataset.to_csv('cleaned_test.txt',columns=[\"text\"],index=False,header=False)"
      ],
      "metadata": {
        "id": "hfgwTkmCMhDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_dataset['train']),len(test_dataset['train'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IHWao4s9Pz40",
        "outputId": "4c992395-f1d3-41c5-ed2c-1e01cb961a1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11618, 2471)"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = load_dataset(\"text\", data_files=\"cleaned_train.txt\")\n",
        "test_dataset = load_dataset(\"text\",data_files=\"cleaned_test.txt\")\n"
      ],
      "metadata": {
        "id": "zmNc3FDkuwtE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_dataset['train']),len(test_dataset['train'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jPw6TB8oQDM9",
        "outputId": "d8ce5bfc-6761-4821-83d2-4c01e6883081"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(13695, 2891)"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Dp1LjS39QBcs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BERT INPUT PREPARATION - NSP & MLM USING CUSTOM TOKENIZER\n"
      ],
      "metadata": {
        "id": "fm6HMizn-jXe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PAD, CLS, SEP, MASK = 0, 1, 2, 3\n",
        "SPECIAL_TOKENS = [\"[PAD]\", \"[CLS]\", \"[SEP]\",\"[EOS]\" ,\"[MASK]\"]\n",
        "\n",
        "def build_vocab(sentences):\n",
        "    words = set([word for sent in sentences for word in sent.lower().split()])\n",
        "    vocab = {word:i+len(SPECIAL_TOKENS)for i,word in enumerate(sorted(words))}\n",
        "    for i, token in enumerate(SPECIAL_TOKENS):\n",
        "      vocab[token] = i\n",
        "    return vocab\n",
        "\n",
        "vocab = build_vocab(train_dataset['train']['text'])\n",
        "len(vocab)"
      ],
      "metadata": {
        "id": "PVhATgUinLSn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02175836-3490-4da6-dba2-0c1c965af5eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "67275"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_bert_batch_manual(examples, max_len=128):\n",
        "    texts = examples['text']\n",
        "    n = len(texts)\n",
        "    vocab_size = len(vocab)\n",
        "    inv_vocab = {v:k for k,v in vocab.items()}\n",
        "    batch_input_ids, batch_segments, batch_masks,batch_mlm, batch_nsp = [],[],[],[],[]\n",
        "    for i in range(n):\n",
        "      # NSP\n",
        "      is_next = 1 if torch.rand(1)>0.5 and i<n-1 else 0\n",
        "      s1 = texts[i].lower().split()\n",
        "      s2 = texts[i + 1 if is_next else torch.randint(0,n,(1,))].lower().split()\n",
        "      while len(s1)+len(s2) > max_len - 3:\n",
        "        if len(s1)>len(s2):\n",
        "          s1.pop()\n",
        "        else:\n",
        "          s2.pop()\n",
        "      tokens = [\"[CLS]\"] + s1 + [\"[SEP]\"] + s2 + [\"[EOS]\"]\n",
        "      segements = [0]*(len(s1)+2) + [1]*(len(s2)+1)\n",
        "\n",
        "      # MLM\n",
        "      input_ids = [vocab.get(t,vocab[\"[PAD]\"]) for t in tokens]\n",
        "      mlm_labels = [-100] * len(input_ids)\n",
        "      for idx,token in enumerate(tokens):\n",
        "        if token in SPECIAL_TOKENS:\n",
        "          continue\n",
        "        if torch.rand(1) < 0.15:  # 15%\n",
        "          mlm_labels[idx] = vocab.get(token,0)\n",
        "          rand_val = torch.rand(1)\n",
        "          if rand_val < 0.8:      # 80% of 15%\n",
        "            input_ids[idx] = MASK\n",
        "          elif rand_val < 0.9:    # other 10% of 15%\n",
        "            input_ids[idx] = torch.randint(len(SPECIAL_TOKENS),n,(1,)).item()\n",
        "          else:\n",
        "            pass # remain same (remaining 10% of 15%)\n",
        "\n",
        "      padding_len = max_len - len(input_ids)\n",
        "      attention_mask = [1]*len(input_ids)+ [0]*padding_len\n",
        "      input_ids += [0]*padding_len\n",
        "      segements += [0]*padding_len\n",
        "      mlm_labels += [-100] * padding_len\n",
        "      batch_input_ids.append(input_ids)\n",
        "      batch_masks.append(attention_mask)\n",
        "      batch_segments.append(segements)\n",
        "      batch_mlm.append(mlm_labels)\n",
        "      batch_nsp.append(is_next)\n",
        "\n",
        "    return {\n",
        "      \"input_ids\": torch.tensor(batch_input_ids),\n",
        "      \"token_type_ids\": torch.tensor(batch_segments),\n",
        "      \"attention_mask\": torch.tensor(batch_masks),\n",
        "      \"labels\": torch.tensor(batch_mlm),\n",
        "      \"next_sentence_label\": torch.tensor(batch_nsp)\n",
        "    }"
      ],
      "metadata": {
        "id": "II_P0jHsnIzj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loader"
      ],
      "metadata": {
        "id": "osM8rV7Ztz0g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "processed_train_dataset = train_dataset.map(prepare_bert_batch_manual,batched=True,remove_columns=train_dataset['train'].column_names)\n",
        "processed_test_dataset = test_dataset.map(prepare_bert_batch_manual,batched=True,remove_columns=test_dataset['train'].column_names)\n",
        "processed_train_dataset = processed_train_dataset['train']\n",
        "processed_test_dataset = processed_test_dataset['train']"
      ],
      "metadata": {
        "id": "2LvAL9XDreZj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "7e124480-1c3d-452e-a445-954e5e53466b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/11618 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6558ee1585a94e5dbb11686a5b201d24"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/2471 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3898f35c753442b0ab55d147329af3ce"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "processed_train_dataset.set_format(type='torch')\n",
        "processed_test_dataset.set_format(type='torch')"
      ],
      "metadata": {
        "id": "SYc9YXTl_sBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(processed_train_dataset,batch_size=16,shuffle=True)\n",
        "test_loader = DataLoader(processed_test_dataset,batch_size=16,shuffle=True)"
      ],
      "metadata": {
        "id": "AGLDwFCtzRBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader"
      ],
      "metadata": {
        "id": "OaQZwEbb_sKg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5f5cf7f-5c32-4e59-b2de-dc00baef6fae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.utils.data.dataloader.DataLoader at 0x7c876935e9c0>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loader"
      ],
      "metadata": {
        "id": "PG-aSXej_sNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BERT : Architecture"
      ],
      "metadata": {
        "id": "6FzYAS6ChUcv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BERT - Embedding Layer"
      ],
      "metadata": {
        "id": "THGUHH2jtSzD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEmbedding(torch.nn.Module):\n",
        "  def __init__(self, d_model=768, max_len=128):\n",
        "    super().__init__()\n",
        "    pe = torch.zeros(max_len, d_model).float()\n",
        "\n",
        "    for pos_idx in range(max_len):\n",
        "      for i in range(0, d_model, 2):\n",
        "        pe[pos_idx, i] = math.sin(pos_idx / (10000 ** ((2 * i) / d_model)))\n",
        "        if i + 1 < d_model:\n",
        "          pe[pos_idx, i + 1] = math.cos(pos_idx / (10000 ** ((2 * i) / d_model)))\n",
        "    self.register_buffer('pe', pe.unsqueeze(0))\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.pe[:, :x.size(1)]"
      ],
      "metadata": {
        "id": "OLfI0BCtFcln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BERTEmbedding(nn.Module):\n",
        "  def __init__(self, vocab_size, hidden_size, max_len, n_segments):\n",
        "    super().__init__()\n",
        "    self.tok_embed = nn.Embedding(vocab_size,hidden_size, padding_idx=0)\n",
        "    self.pos_embed = PositionalEmbedding(hidden_size,max_len)\n",
        "    self.seg_embed = nn.Embedding(n_segments,hidden_size,padding_idx=0)\n",
        "    self.norm = nn.LayerNorm(hidden_size)\n",
        "    self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "  def forward(self, x, seg):\n",
        "    # self.pos_embed(x) internally uses the buffer 'pe'\n",
        "    # which PyTorch moved to CUDA when you did model.to(device)\n",
        "    embed = self.tok_embed(x) + self.pos_embed(x) + self.seg_embed(seg)\n",
        "    return self.dropout(embed)"
      ],
      "metadata": {
        "id": "MFhRp07ytTmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BERT Class"
      ],
      "metadata": {
        "id": "qiIdYrKthZr3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BERT(nn.Module):\n",
        "  def __init__(self,vocab_size, hidden_size=768, n_layers=12, n_heads=12, max_len=128):\n",
        "    super().__init__()\n",
        "    self.embedding = BERTEmbedding(vocab_size,hidden_size,max_len,n_segments=2)\n",
        "\n",
        "    encoder_layer = nn.TransformerEncoderLayer(\n",
        "        d_model = hidden_size,\n",
        "        nhead = n_heads,\n",
        "        batch_first = True,\n",
        "        activation = 'gelu',\n",
        "        norm_first = True\n",
        "    )\n",
        "\n",
        "    self.encoder = nn.TransformerEncoder(\n",
        "        encoder_layer,\n",
        "        num_layers=n_layers\n",
        "\n",
        "    )\n",
        "\n",
        "    self.fc = nn.Linear(hidden_size,hidden_size)\n",
        "    self.activ = nn.Tanh()\n",
        "    self.classifier = nn.Linear(hidden_size,2)\n",
        "    self.mlm_head = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "  def forward(self, x, seg, attention_mask):\n",
        "    x = self.embedding(x,seg)\n",
        "    # src_key_padding_mask = (attention_mask==0)\n",
        "    src_key_padding_mask = (attention_mask == 0).to(torch.bool).to(device)\n",
        "    # mask = (x > 0).unsqueeze(1).repeat(1, x.size(1), 1).unsqueeze(1)รท\n",
        "    encoded = self.encoder(x, src_key_padding_mask = src_key_padding_mask)\n",
        "    # MLM\n",
        "    mlm_output = self.mlm_head(encoded)\n",
        "    # NSP\n",
        "    cls_token = encoded[:,0]\n",
        "    cls_output = self.activ(self.fc(cls_token))\n",
        "    nsp_logits = self.classifier(cls_output)\n",
        "    return mlm_output, nsp_logits\n",
        "  def _init_weights(self,module):\n",
        "    if isinstance(module, (nn.Linear, nn.Embedding)):\n",
        "        module.weight.data.normal_(mean=0.0, std=0.02)\n",
        "    if isinstance(module, nn.Linear) and module.bias is not None:\n",
        "        module.bias.data.zero_()"
      ],
      "metadata": {
        "id": "CnVhf1L_xDjG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training BERT"
      ],
      "metadata": {
        "id": "MJXQKwRZ7oXh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "vocab_size = len(vocab)\n",
        "model = BERT(vocab_size)\n",
        "model.to(device) # Move the model to the specified device\n",
        "model.apply(model._init_weights)\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=1e-5,weight_decay=0.01)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=-100)"
      ],
      "metadata": {
        "id": "wTOnhlsExDra",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05777f94-0d44-4207-c591-abb6585a0a2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_save(model,train_loader,optimizer,device,epochs=5):\n",
        "  history = {\n",
        "      \"total_loss\":[],\n",
        "      \"mlm_loss\":[],\n",
        "      \"nsp_loss\":[]\n",
        "  }\n",
        "  model.train()\n",
        "  print(\"Starting training\")\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    epoch_total,epoch_mlm,epoch_nsp = 0,0,0\n",
        "    loop = tqdm(train_loader,leave=True)\n",
        "    for batch in loop:\n",
        "      optimizer.zero_grad()\n",
        "      input_ids = batch['input_ids'].to(device)\n",
        "      token_type_ids = batch['token_type_ids'].to(device)\n",
        "      attention_mask = batch['attention_mask'].to(device)\n",
        "      mlm_labels = batch['labels'].to(device)\n",
        "      nsp_labels = batch['next_sentence_label'].to(device)\n",
        "      mlm_logits,nsp_logits = model(input_ids,token_type_ids,attention_mask)\n",
        "      loss_mlm = criterion(mlm_logits.view(-1,vocab_size),mlm_labels.view(-1))\n",
        "      loss_nsp = criterion(nsp_logits, nsp_labels)\n",
        "      loss = loss_mlm + loss_nsp\n",
        "\n",
        "      loss.backward()\n",
        "      torch.nn.utils.clip_grad_norm_(model.parameters(),1.0)\n",
        "      optimizer.step()\n",
        "\n",
        "      epoch_total += loss.item();\n",
        "      epoch_mlm += loss_mlm.item()\n",
        "      epoch_nsp += loss_nsp.item()\n",
        "      loop.set_description(f\"Epoch {epoch+1}/{epochs}\")\n",
        "      loop.set_postfix(mlm=loss_mlm.item(), nsp=loss_nsp.item(),loss=loss.item())\n",
        "    history['total_loss'].append(epoch_total/len(train_loader))\n",
        "    history['mlm_loss'].append(epoch_mlm/len(train_loader))\n",
        "    history['nsp_loss'].append(epoch_nsp/len(train_loader))\n",
        "  torch.save(model.state_dict(),\"bert_pretrained2.pth\")\n",
        "  with open(\"loss_history2.json\", \"w\") as f:\n",
        "    json.dump(history, f)\n",
        "  print(\"Model saved\")\n",
        "  return history"
      ],
      "metadata": {
        "id": "ftlKlkkIxDuA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = train_and_save(model,train_loader,optimizer,device)"
      ],
      "metadata": {
        "id": "TlCJp63zzvkj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_losses(history):\n",
        "  epochs = range(1, len(history[\"total_loss\"]) + 1)\n",
        "\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  plt.plot(epochs, history[\"mlm_loss\"], 'b-o', label='MLM Loss')\n",
        "  plt.plot(epochs, history[\"nsp_loss\"], 'r-o', label='NSP Loss')\n",
        "  plt.plot(epochs, history[\"total_loss\"], 'g--', label='Total Loss')\n",
        "\n",
        "  plt.title('BERT Pre-training Loss')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.legend()\n",
        "  plt.grid(True)\n",
        "  plt.show()\n",
        "\n",
        "plot_losses(history)"
      ],
      "metadata": {
        "id": "oxHhtUiUz-I0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BERT(vocab_size=len(tokenizer), hidden_size=768, n_layers=12)\n",
        "model.load_state_dict(torch.load(\"bert_model.pth\", map_location=device))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "with open(\"loss_history.json\", \"r\") as f:\n",
        "    history = json.load(f)"
      ],
      "metadata": {
        "id": "PixdUvhI5Tju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = train_and_save(model,train_loader,optimizer,device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "id": "RGnUWBEHfP3R",
        "outputId": "2172ad32-aa30-4fbb-fbfb-1c68f7af1324"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/856 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ee9c9e8c90384accb13d50cb160281d0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/856 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c526eafde4d84a72a0bbc581eb82cbd5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/856 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c2835786ad9442488dbc68ef6d5eedce"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/856 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "04a5dd487e164df886064523c4fadc53"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/856 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ad03e37a2fe44eb294d496b5901fec2b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved\n"
          ]
        }
      ]
    }
  ]
}